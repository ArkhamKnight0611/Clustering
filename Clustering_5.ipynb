{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Contingency Matrix and Classification Model Evaluation\n",
        "\n",
        "A contingency matrix, also known as a confusion matrix, is a table that visually represents the performance of a classification model. It shows how many data points were classified correctly (True Positives, True Negatives) and incorrectly (False Positives, False Negatives) for each class. This helps evaluate the model's ability to distinguish between different categories.\n",
        "\n",
        "Q2. Pair Confusion Matrix vs. Regular Confusion Matrix\n",
        "\n",
        "A regular confusion matrix compares all possible classifications. A pair confusion matrix, however, focuses on a specific pair of classes. It's useful in situations where a model struggles to differentiate between two particular classes, allowing for a more detailed analysis of those specific errors.\n",
        "\n",
        "Q3. Extrinsic Measures in NLP\n",
        "\n",
        "Extrinsic measures assess how well a language model performs within a real-world application. They don't directly evaluate the model's internal workings but rather its impact on the overall task. Examples include:\n",
        "\n",
        "Machine translation quality: How well does the model translate text into another language, judged by human experts or metrics like BLEU score.\n",
        "Question answering accuracy: Can the model answer questions accurately based on a given context?\n",
        "Chatbot performance: Does the model effectively communicate and fulfill user needs in a chatbot setting?\n",
        "Q4. Intrinsic vs. Extrinsic Measures in Machine Learning\n",
        "\n",
        "Extrinsic measures, as mentioned above, assess a model's performance in a specific application.\n",
        "\n",
        "Intrinsic measures, on the other hand, evaluate the model's ability to learn the underlying patterns in the data itself, independent of any specific application. They don't necessarily reflect real-world effectiveness. Examples include:\n",
        "\n",
        "Accuracy: Proportion of correctly classified data points (for classification tasks).\n",
        "Mean squared error (MSE): Average squared difference between predicted and actual values (for regression tasks).\n",
        "Q5. Confusion Matrix and Model Strengths/Weaknesses\n",
        "\n",
        "A confusion matrix helps identify a model's strengths and weaknesses by:\n",
        "\n",
        "High True Positives/Negatives: Indicates good performance for those classes.\n",
        "High False Positives: The model is incorrectly classifying instances from another class as this class.\n",
        "High False Negatives: The model is missing instances that belong to this class.\n",
        "By analyzing these imbalances, you can identify which classes the model struggles with and focus on improving its performance in those areas.\n",
        "\n",
        "Q6. Intrinsic Measures for Unsupervised Learning\n",
        "\n",
        "Unsupervised learning doesn't have predefined classes. Here are common intrinsic measures:\n",
        "\n",
        "Silhouette Coefficient: Measures how well data points are clustered within their assigned clusters compared to those in neighboring clusters. Values closer to 1 indicate better clustering.\n",
        "Calinski-Harabasz Index: Similar to Silhouette Coefficient, it compares the variance within clusters to the variance between clusters. Higher values indicate better separation.\n",
        "Davies-Bouldin Index: Measures the ratio of the within-cluster scatter to the distance between cluster centers. Lower values indicate better clustering.\n",
        "Q7. Limitations of Accuracy and Addressing Them\n",
        "\n",
        "Accuracy, the proportion of correctly classified data points, has limitations:\n",
        "\n",
        "Imbalanced datasets: If one class dominates, accuracy can be misleading. A model might perform well on the majority class but poorly on the minority class.\n",
        "Class-independent costs: Not all misclassifications are equal. Accuracy treats all errors the same, while in reality, some errors might be more critical than others.\n",
        "Here's how to address these limitations:\n",
        "\n",
        "Use balanced accuracy: Consider the accuracy for each class individually.\n",
        "Use F1-score: Combines precision (proportion of true positives among predicted positives) and recall (proportion of true positives among actual positives) to account for both types of errors.\n",
        "Cost-sensitive learning: Assign weights to different types of errors based on their real-world impact."
      ],
      "metadata": {
        "id": "TOcHhEIHOz0w"
      }
    }
  ]
}